{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1840101c",
   "metadata": {},
   "source": [
    "### Advanced Models for ETTh1 Forecasting Benchmark\n",
    "\n",
    "This example demonstrates advanced forecasting models applied to the ETTh1 (Electricity Transformer Temperature, Hourly) benchmark, a widely adopted standard in the forecasting community. The ETTh1 dataset presents significant challenges with its high-frequency electricity transformer measurements, capturing complex temporal dynamics including daily and seasonal cycles, alongside irregular fluctuations that test state-of-the-art forecasting capabilities.\n",
    "\n",
    "To keep computations efficient in this notebook, we use a 1,000-row subset and smaller `forecasting_horizon` and `observation_length` values than in standard benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00050d3",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e2cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(pathlib.Path().resolve().parent.as_posix())\n",
    "\n",
    "from inait import predict_test, score_test, plot, read_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea1e14",
   "metadata": {},
   "source": [
    "### Load the data and split it into train/test \n",
    "\n",
    "The ETTh1 dataset (source: Zhou, Haoyi, et al. [\"Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\"](https://arxiv.org/pdf/2012.07436), 2021.) contains hourly electricity transformer measurements for seven variables, including electricity loads and temperatures. Our goal is to predict all seven variables simultaneously for the next 12 hours using three increasingly sophisticated models: `inait-basic`, `inait-advanced`, and `inait-best`. Keep in mind that higher-performing models come with longer computation times.\n",
    "\n",
    "Model evaluation follows standard machine learning practices: a portion of the dataset is held out as a test set for performance assessment, while the rest is used for training. The test set remains unseen during training to provide an unbiased evaluation. We measure performance using Mean Absolute Error (MAE); lower MAE indicates better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/etth1_small.csv\"\n",
    "data = read_file(data_path, index_col=0)\n",
    "plot(historical_data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure prediction parameters\n",
    "target_columns = data.columns.tolist()  # Use all columns as targets\n",
    "\n",
    "forecasting_horizon = 12  # Predict 12 hours ahead\n",
    "observation_length = 24  # Use last 24 hours as historical context\n",
    "\n",
    "test_size = 5  # we will evaluate the model performances on the last 5 steps\n",
    "\n",
    "models = [\"inait-basic\", \"inait-advanced\", \"inait-best\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502db362",
   "metadata": {},
   "source": [
    "**Note:** The next cell may take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c6613",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, predictions = {}, {}\n",
    "for model in tqdm(models, leave=True, postfix=f\"Evaluating models {', '.join(models)}\"):\n",
    "    prediction = predict_test(\n",
    "        data=data,\n",
    "        target_columns=target_columns,\n",
    "        forecasting_horizon=forecasting_horizon,\n",
    "        observation_length=observation_length,\n",
    "        model=model,\n",
    "        test_size=test_size,\n",
    "    )[\"predictions\"]\n",
    "    predictions[model] = prediction\n",
    "    scores[model] = score_test(predictions=prediction, ground_truth=data, metric=\"mae\")\n",
    "    time.sleep(1)\n",
    "\n",
    "scores_df = pd.DataFrame.from_dict(scores, orient=\"index\", columns=[\"MAE\"])\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d28a02",
   "metadata": {},
   "source": [
    "### Comparison against open-source baseline models\n",
    "\n",
    "From the Mean Absolute Error values of the three models shown above, we can already see that more complex approaches tend to yield better results.\n",
    "\n",
    "To put our results into perspective, we compare the inait models against traditional forecasting baselines implemented with open-source libraries. We evaluate two common baselines:\n",
    "- Naive model; simply repeats the last observed time step.\n",
    "- Linear regression model; fits a linear relationship to past observations.\n",
    "\n",
    "While these tools are freely available, implementing them effectively still requires solid forecasting and data science expertise as you'll notice from the multiple lines of code in the next cell.\n",
    "\n",
    "\n",
    "**Note:** These notebooks are optimized for fast execution. If you notice delays with the scikit-learn model, it's likely due to the shared MyBinder server used for this zero-setup demo. To ensure quick results, we load pre-computed Linear model predictions; if you prefer to run the computations yourself, simply set the `use_precomputed_predictions` flag to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "use_precomputed_predictions = True  # Change to False if running locally\n",
    "\n",
    "\n",
    "class NaiveBaseline(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Naive baseline that repeats the last observed value for each target\"\"\"\n",
    "\n",
    "    def __init__(self, strategy=\"last\"):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # For naive baseline, we don't need to fit anything\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.strategy == \"last\":\n",
    "            # X shape: (n_samples, obs_len * n_features)\n",
    "            n_samples, n_features_flat = X.shape\n",
    "            n_features = len(target_columns)  # Assuming target_columns is available\n",
    "            obs_len = n_features_flat // n_features\n",
    "\n",
    "            # Reshape X to (n_samples, obs_len, n_features)\n",
    "            X_reshaped = X.reshape(n_samples, obs_len, n_features)\n",
    "\n",
    "            # Take the last observation for each sample (shape: n_samples, n_features)\n",
    "            last_obs = X_reshaped[:, -1, :]  # (n_samples, n_features)\n",
    "\n",
    "            # Repeat the last observation for forecasting_horizon steps\n",
    "            predictions = np.tile(\n",
    "                last_obs[:, None, :], (1, forecasting_horizon, 1)\n",
    "            )  # (n_samples, forecasting_horizon, n_features)\n",
    "\n",
    "            # Flatten to (n_samples, forecasting_horizon * n_features)\n",
    "            predictions = predictions.reshape(\n",
    "                n_samples, forecasting_horizon * n_features\n",
    "            )\n",
    "\n",
    "            return predictions\n",
    "\n",
    "        return np.zeros((X.shape[0], X.shape[1]))  # Fallback\n",
    "\n",
    "\n",
    "def predict_sklearn(\n",
    "    data,\n",
    "    target_columns,\n",
    "    forecasting_horizon,\n",
    "    observation_length,\n",
    "    estimator=None,\n",
    "    train_size=None,\n",
    "    test_size=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Forecast using sklearn estimators with column-wise window standardization\n",
    "    \"\"\"\n",
    "    if estimator == \"naive\":\n",
    "        estimator = NaiveBaseline()\n",
    "    else:\n",
    "        estimator = LinearRegression(tol=0.001)\n",
    "\n",
    "    if train_size is not None and test_size is not None:\n",
    "        raise ValueError(\n",
    "            \"Both train_size and test_size cannot be specified at the same time. Please specify only one of them.\"\n",
    "        )\n",
    "    if train_size is not None:\n",
    "        split_idx = int(len(data) * train_size)\n",
    "    elif test_size is not None:\n",
    "        split_idx = len(data) - test_size - forecasting_horizon\n",
    "    else:\n",
    "        train_size = 0.8  # Default to 80% training data\n",
    "        split_idx = int(len(data) * train_size)\n",
    "\n",
    "    train_data = data.iloc[:split_idx]\n",
    "\n",
    "    # Create sequences from training data with column-wise standardization\n",
    "    def create_sequences(data, obs_len, horizon):\n",
    "        X, y = [], []\n",
    "\n",
    "        for i in range(len(data) - obs_len - horizon + 1):\n",
    "            # Get observation window\n",
    "            window = data.iloc[i : i + obs_len].values\n",
    "\n",
    "            X.append(window.flatten())\n",
    "            y.append(data.iloc[i + obs_len : i + obs_len + horizon].values)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    # Train model on training sequences only\n",
    "    X_train, y_train = create_sequences(\n",
    "        train_data[target_columns], observation_length, forecasting_horizon\n",
    "    )\n",
    "    y_train_flat = y_train.reshape(y_train.shape[0], -1)\n",
    "\n",
    "    # Fit model\n",
    "    if isinstance(estimator, NaiveBaseline):\n",
    "        model = estimator\n",
    "        model.fit(X_train, y_train_flat)\n",
    "    else:\n",
    "        model = MultiOutputRegressor(estimator)\n",
    "        model.fit(X_train, y_train_flat)\n",
    "\n",
    "    # Generate predictions for test period\n",
    "    predictions = []\n",
    "    start_test_idx = split_idx\n",
    "\n",
    "    for t in tqdm(range(start_test_idx + 1, len(data) - forecasting_horizon + 1)):\n",
    "        # Get test window\n",
    "        test_window = data.iloc[t - observation_length : t].values\n",
    "\n",
    "        X_test = test_window.flatten().reshape(1, -1)\n",
    "\n",
    "        y_pred_flat = model.predict(X_test)\n",
    "        y_pred = y_pred_flat.reshape(forecasting_horizon, len(target_columns))\n",
    "\n",
    "        # Create prediction DataFrame\n",
    "        pred_df = pd.DataFrame(\n",
    "            y_pred,\n",
    "            columns=target_columns,\n",
    "            index=data.index[t : t + forecasting_horizon],\n",
    "        )\n",
    "        predictions.append(pred_df)\n",
    "\n",
    "    estimator_name = (\n",
    "        \"naive\" if isinstance(estimator, NaiveBaseline) else type(estimator).__name__\n",
    "    )\n",
    "    session_ids = [f\"{estimator_name}_session_{i}\" for i in range(len(predictions))]\n",
    "\n",
    "    return predictions, session_ids\n",
    "\n",
    "\n",
    "# Naive Baseline\n",
    "print(\"Running Naive model...\")\n",
    "naive_predictions, naive_sessions = predict_sklearn(\n",
    "    data=data,\n",
    "    target_columns=target_columns,\n",
    "    forecasting_horizon=forecasting_horizon,\n",
    "    observation_length=observation_length,\n",
    "    estimator=\"naive\",\n",
    "    test_size=test_size,\n",
    ")\n",
    "\n",
    "\n",
    "predictions[\"Naive from scratch\"] = [\n",
    "    df.add_suffix(\"_predicted\") for df in naive_predictions\n",
    "]\n",
    "\n",
    "if use_precomputed_predictions:\n",
    "    print(\"Loading pre-computed predictions for Linear model...\")\n",
    "    linear_predictions = [\n",
    "        pd.read_parquet(f\"../assets/nb_1_linear_prediction_{i}.parquet\")\n",
    "        for i in range(len(naive_predictions))\n",
    "    ]\n",
    "else:\n",
    "    # Linear Regression\n",
    "    print(\"Running Linear model...\")\n",
    "    linear_predictions, linear_sessions = predict_sklearn(\n",
    "        data=data,\n",
    "        target_columns=target_columns,\n",
    "        forecasting_horizon=forecasting_horizon,\n",
    "        observation_length=observation_length,\n",
    "        estimator=LinearRegression(),\n",
    "        test_size=test_size,\n",
    "    )\n",
    "    # for i, linear_prediction in enumerate(linear_predictions):\n",
    "    #     linear_prediction.to_parquet(f\"../assets/nb_1_linear_prediction_{i}.parquet\")\n",
    "\n",
    "# Concatenate predictions and scores into a single DataFrame to match the format of inait predictions\n",
    "predictions[\"Linear from scratch\"] = [\n",
    "    df.add_suffix(\"_predicted\") for df in linear_predictions\n",
    "]\n",
    "\n",
    "scores_df = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            score_test(predictions=naive_predictions, ground_truth=data, metric=\"mae\"),\n",
    "            columns=[\"MAE\"],\n",
    "            index=[\"Naive from scratch\"],\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            score_test(predictions=linear_predictions, ground_truth=data, metric=\"mae\"),\n",
    "            columns=[\"MAE\"],\n",
    "            index=[\"Linear from scratch\"],\n",
    "        ),\n",
    "        pd.DataFrame.from_dict(scores, orient=\"index\", columns=[\"MAE\"]),\n",
    "    ],\n",
    "    axis=0,\n",
    ").round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a712f122",
   "metadata": {},
   "source": [
    "### Performance comparison visualization\n",
    "\n",
    "Let us look at the Mean Absolute Error for all inait and open-source models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df97931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Create vertical bar plot with green color scale based on values\n",
    "fig = px.bar(\n",
    "    scores_df,\n",
    "    x=scores_df.index,\n",
    "    y=\"MAE\",\n",
    "    color=\"MAE\",\n",
    "    color_continuous_scale=\"Greens_r\",  # Reversed greens (darker for lower values)\n",
    "    title=\"Model Performance Comparison\",\n",
    "    labels={\"x\": \"Models\", \"y\": \"MAE\"},\n",
    "    text=\"MAE\",  # Add values on bars\n",
    ")\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Models\",\n",
    "    yaxis_title=\"MAE (lower is better)\",\n",
    "    showlegend=False,\n",
    "    yaxis=dict(\n",
    "        range=[\n",
    "            scores_df[\"MAE\"].min() * 0.9,\n",
    "            scores_df[\"MAE\"].max() * 1.1,\n",
    "        ]\n",
    "    ),\n",
    "    coloraxis_showscale=False,\n",
    ")\n",
    "\n",
    "fig.update_traces(texttemplate=\"%{text:.4f}\", textposition=\"outside\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8dfe12",
   "metadata": {},
   "source": [
    "The plot below compares the inait models with open-source baseline implementations. For clarity, we show only the last prediction for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    historical_data=data.loc[\n",
    "        : predictions[models[0]][-1].index[-1], :\n",
    "    ],  # Show all historical data up to the last prediction\n",
    "    predicted_data={\n",
    "        key: values[-1] for key, values in predictions.items()\n",
    "    },  # Get the last test set for each model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561ffc5",
   "metadata": {},
   "source": [
    "Let us now look at performances in terms of Mean Absolute Error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f87b1",
   "metadata": {},
   "source": [
    "### Comments on results\n",
    "\n",
    "The models successfully capture the ground truth patterns for most variables, demonstrating strong forecasting performance. While some variables show larger prediction errors, it's important to note that this is a simplified simulation optimized for notebook execution time.\n",
    "\n",
    "Running the full benchmark configuration would yield the comprehensive results shown in the following comparison plot against state-of-the-art pretrained models from leading competitors: [NXAI (TiRex)](https://github.com/NX-AI/tirex), [IBM (Tiny Time Mixers)](https://research.ibm.com/publications/tiny-time-mixers-ttms-fast-pre-trained-models-f[…]nced-zerofew-shot-forecasting-of-multivariate-time-series--1), [NIXTLA (TimeGPT)](https://www.nixtla.io/docs/introduction/introduction).\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../assets/benchmark_etth1_inait.png\" alt=\"Benchmark comparison results\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61781891",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inait-predict-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
